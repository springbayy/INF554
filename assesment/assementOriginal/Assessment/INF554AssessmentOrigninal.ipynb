{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15add333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01544a5b",
   "metadata": {},
   "source": [
    "# INF554 Assessment: Trees, Bagging and Boosting\n",
    "\n",
    "In this assessment it is your task to delve deeper into the construction of classification tree-based models.\n",
    "\n",
    "**Please submit a zip file containing the following to moodle by the 30th October 2023 14.00 (Paris time):**\n",
    "\n",
    "- your filled out assessment jupyter notebook on the proposed coding tasks;\n",
    "- a pdf containing your short written answers to the questions in this notebook.\n",
    "\n",
    "Make sure to submit a couple of hours ahead of the deadline to ensure that technical difficulties do not cause you to miss the deadline. Late submissions will not be accepted.\n",
    "\n",
    "Please note that this assessment is *to be completed individually. We will forward detected cases of plagarism to the university,* which in serious cases can have farreaching consequences for you. So, please make sure to submit your own, original solutions to this assessment. \n",
    "\n",
    "*Disclaimer:* You will not receive marks for importing the functions that you are asked to code from any library, unless specifically stated in the corresponding task. In this assessment, we ask you to code these methods from scratch in the hope that this will give you a better understanding of them.\n",
    "\n",
    "## Data set\n",
    "\n",
    "In this assessment, you will be working with the <a href=\"https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\">Pima-indians-diabetes</a> dataset which originates from a study that aimed to predict the presence (Outcome=1) or not (Outcome=0) of diabetes in 768 female patients who were at least 21 years old and had Pima Indian origin. For this purpose, the following diagnostic measures were considered: number of pregnancies (Pregnancies, $X_1$), plasma glucose concentration (Glucose, $X_2$), diastolic blood pressure (BloodPressure, $X_3$), triceps skinfold thickness (SkinThickness, $X_4$), serum insulin (Insulin, $X_5$), Ìbody mass index (BMI, $X_6$), diabetes pedigree function (DiabetesPedigreeFunction, $X_7$) and age (Age, $X_8$). Thus, we have 8 predictor variables and the binary response variable Outcome, which we denote by $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "print(df.head())\n",
    "features = df.columns[0:(len(df.columns)-1)]\n",
    "X = np.array(df[features])\n",
    "y = np.array(df.Outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3c80a",
   "metadata": {},
   "source": [
    "## A. Trees\n",
    "\n",
    "In this section, we will learn how to construct a classification tree. Suppose we are given a training sample of $N$ individuals, $\\left\\lbrace \\left(x_i,y_i \\right) \\right\\rbrace_{1\\leq i\\leq N}$, on which $d$ predictor variables are measured, $x_i\\in\\mathbb{R}^d$, and one class label is associated with each one, $y_i\\in\\left\\lbrace 1,\\ldots, K\\right\\rbrace$. Classic classification tree methods consist of sequentially, and greedily, partitioning the predictor space $\\mathbb{R}^d$ into disjoint sets or *nodes* by imposing certain conditions on the predictor variables. The usual splitting criterion is to take the split that makes descendant nodes purer, i.e., nodes with observations more and more homogenous in terms of class membership. The process of partitioning finishes when a stopping criterion is satisfied. Then, leaf nodes are labeled with a class label, $1,\\ldots,K$. Commonly, a leaf node is labeled with the most frequent class in the individuals that have fallen into the node. Once the tree is built, the prediction of future unlabeled data is done in a deterministic way. Given a new observation, starting from the root node, it will end up in a leaf node, depending on the values the predictor variables take, and its predicted class will be the class label of that leaf node. Because of their construction, normalization of the data is not required.\n",
    "\n",
    "Specifically, we will focus on binary classification trees with univariate splits. Given a node $t$, the proposed splitting for continuous features is to generate the following left and right child nodes: $t_L = \\left\\lbrace u\\in t: x_{ju} \\leq b_t\\right\\rbrace$ and $t_R = \\left\\lbrace u\\in t: x_{ju} > b_t\\right\\rbrace$, where cutpoint $b_t$ is the halfway between two consecutive data values of feature $x_j$. Assuming that these values are ordered from lowest to highest, at most $N-1$ different divisions can be done. As said, the best split will be the one that best separate the objects in the training sample up to their classes, that is, that produces the maximum reduction in the diversity or impurity of objects associated to resultant nodes. In the following, we study different quantitative ways to measure the impurity of a node.\n",
    "\n",
    "### Node impurity measures\n",
    "\n",
    "Given node $t$, an impurity measure $i(t)$ can be defined by:\n",
    "\n",
    "**Misclassification:** $1-\\max_\\limits{1\\leq k\\leq K} \\left\\lbrace p_k(t)\\right\\rbrace$\n",
    "\n",
    "**Entropy:** $-\\sum_\\limits{k=1}^K p_k(t) \\log_2 p_k(t)$, with the agreement that $0\\log_2 0 = 0$.\n",
    "\n",
    "**Gini-index:** $\\sum_\\limits{k=1}^K p_k(t) \\left( 1-p_k(t)\\right) = 1 - \\sum_\\limits{k=1}^K p_k^2(t)$\n",
    "\n",
    "where $p_k(t) = \\dfrac{n_k(t)}{n(t)}$ is the proportion of samples in node $t$ that belong to class $k$.\n",
    "\n",
    ">**Task 1: (3 Points)** Complete the below functions to define the three impurity measures above. In addition, plot them together as a function of $p_1(t)$ for a given binary classification problem ($K=2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification(p):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        p (K-dimensional vector): class probability distribution at a given node t\n",
    "    Outputs:\n",
    "        i (float): misclassification impurity measure at node t\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        p (K-dimensional vector): class probability distribution at a given node t\n",
    "    Outputs:\n",
    "        i (float): entropy impurity measure at node t\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "\n",
    "def gini_index(p):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        p (K-dimensional vector): class probability distribution at a given node t\n",
    "    Outputs:\n",
    "        i (float): Gini index impurity measure at node t\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "    \n",
    "plt.figure()\n",
    "x = np.linspace(0,1,1000)\n",
    "y1 = [misclassification([p1,1-p1]) for p1 in x]\n",
    "plt.plot(x,y1)\n",
    "y2 = [entropy([p1,1-p1]) for p1 in x] \n",
    "plt.plot(x,y2)\n",
    "y3 = [gini_index([p1,1-p1]) for p1 in x]\n",
    "plt.plot(x,y3)\n",
    "plt.legend([\"Misclassification\",\"Entropy\",\"Gini index\"])\n",
    "plt.xlabel(\"Probability of Class 1\")\n",
    "plt.ylabel(\"Node Impurity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0149a2",
   "metadata": {},
   "source": [
    ">**Question 1: (6 Points)** Impurity measures are functions $\\Phi: P \\rightarrow \\mathbb{R}$, where $P=\\left\\lbrace \\left(p_1,\\ldots,p_m\\right): \\sum\\limits_{k=1}^K p_k = 1, p_k\\geq 0, k=1,\\ldots,K\\right\\rbrace$, that verify the following three properties:\n",
    "> * $\\Phi$ achieves its unique maximum at point $\\left(\\dfrac{1}{K},\\ldots,\\dfrac{1}{K}\\right)$\n",
    "> * $\\Phi$ achieves its minima exclusively at the points $\\left(1,0,\\ldots,0\\right)$, $\\left(0,1,0,\\ldots,0\\right)$, ..., $\\left(0,\\ldots,0,1\\right)$\n",
    "> * $\\Phi$ is a symmetric function of $p_1,\\ldots,p_K$, i.e., if there is a permutation of the variables $p_k$, the function's output will remain unchanged.\n",
    "> 1. Interpret what these three functional properties of impurity measures individually mean in the context of measuring node impurity. (At most 2 sentences per property are required.)\n",
    "> 2. Show analytically for binary classification ($K=2$) that the Gini index satisfies these properties. \n",
    "\n",
    "Since the fundamental idea is to produce purer and purer nodes, the selection of the split of a parent node will be done in terms of the information gain, which measures somehow the purity gained when a node is split into child nodes. In what follows, this concept is formally defined.\n",
    "\n",
    "### Information gain\n",
    "\n",
    "Let $s\\in S$ be a candidate split of data points assigned to a given node $t$, and let $i$ be an impurity function. The information gain of $s$ relative to node $t$ is defined as the average reduction of impurity obtained by splitting the observations within node $t$ up to $s$\n",
    "\n",
    "$$ G(t,s) = i(t) - \\left(q_L i\\left(t_L\\right) + q_R i\\left(t_R\\right)\\right)$$\n",
    "\n",
    "where $t_L, t_R$ are the left and right child nodes originating from the splitting of node $t$ and $q_L, q_R$ the proportion of observations (within node $t$) that become elements from new nodes $t_L, t_R$. This definition could be generalized for non-binary splitting.\n",
    "\n",
    "Let $\\Omega_c$ be the set of current leaf nodes that can be potentially split. Then, the selected split over $\\Omega_c$ will be the one that maximizes the corresponding information gain:\n",
    "\n",
    "$$ G(t^\\ast, s^\\ast) = \\max\\limits_{t\\in\\Omega_c,\\,s\\in S}\\left\\lbrace G(t,s) \\right\\rbrace$$\n",
    "\n",
    ">**Task 2: (9 Points)** Complete the below function following the steps within it in order to produce the first split in a binary classification tree for the first 100 observations of the Pima-indians-diabetes data set and for all three impurity functions. Consider just Glucose as predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "feature = np.array(X[0:100,1])\n",
    "labels = y[0:i]\n",
    "\n",
    "def first_split(feature, labels, impurity_function):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        feature (N-dimensional vector): feature values\n",
    "        labels (N-dimensional vector): target values\n",
    "        impurity_function (function): misclassification, entropy or gini_index functions\n",
    "    Outputs:\n",
    "        impurity_parent_node (float): impurity measure of parent node t\n",
    "        impurity_left_child_node (float): impurity measure of left child node t_L\n",
    "        impurity_right_child_node (float): impurity measure of right child node t_R\n",
    "        best_cutpoint (float): feature value from which to split b_t\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the list of candidate cutpoints\n",
    "    # insert your code here\n",
    "\n",
    "    # Step 2: Compute the information gain for each candidate cutpoint\n",
    "    # insert your code here\n",
    "    \n",
    "    # Step 3: Obtain the first split according to the maximum information gain\n",
    "    # insert your code here\n",
    "    \n",
    "    print('Impurity function used is:', impurity_function.__name__)\n",
    "    print('Impurity at root node =', np.around(impurity_parent_node,3))\n",
    "    print('The left branch is','X <=', best_cutpoint, 'with impurity =', np.around(impurity_left_child_node,3))\n",
    "    print('The right branch is','X >', best_cutpoint, 'with impurity =', np.around(impurity_right_child_node,3))\n",
    "    print('\\n')\n",
    "    \n",
    "    return impurity_parent_node, impurity_left_child_node, impurity_right_child_node, best_cutpoint\n",
    "\n",
    "first_split(feature, labels, misclassification);\n",
    "first_split(feature, labels, entropy);\n",
    "first_split(feature, labels, gini_index);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b99d0",
   "metadata": {},
   "source": [
    ">**Task 3: (2 Points)** Use DecisionTreeClassifier from scikit-learn package to learn the first split for both entropy and Gini index. Are the results consistent with the output obtained in Task 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ba9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "plt.figure()\n",
    "plot_tree(clf_entropy);\n",
    "\n",
    "# insert your code here\n",
    "\n",
    "plt.figure()\n",
    "plot_tree(clf_gini);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5d65c",
   "metadata": {},
   "source": [
    "\n",
    "## B. Bagging Trees\n",
    "\n",
    "Single decision trees are known to suffer from high variance. This means that if two decision trees are grown over two disjoint subsamples from the training data, they may lead to quite different results. A general procedure for reducing variance of any learning method is bootstrap aggregating or bagging. *A bootstrap is a set of observations extracted randomly from the original sample, with replacement and of the same size.* Bagging is a methodology that generates multiple bootstrap samples and the classifier is trained with each one and then the outcomes are aggregated. \n",
    "\n",
    ">**Question 2: (2 Points)** Compute the probability that a given observation in a data set of size N is part of a bootstrap sample, and study the limit of this probability as N tends to infinity. \n",
    "\n",
    "In other words, Question 2 is telling us that approximately one-third of the sample, called set of out-of-bag (OOB) observations, is not being used each time a tree is constructed. These OOB observations will be helpful to compute interesting measures while fitting the model, as we will see in Task 5.\n",
    "\n",
    "A special case of bagging is the well-known Random Forest model. In the Random Forest model, B decision trees are grown over B bootstrapped training samples and the prediction is the class with majority vote; but additionally, at each time the split at node in a given decision tree is considered, a sample of $m$ predictor variables is chosen randomly among the $d$ initial ones. This randomization helps to decorrelate the trees that are being generated.\n",
    "\n",
    "An advantage of bagging is that all the trees are grown independently, and hence they can be fit in parallel.\n",
    "\n",
    ">**Task 4: (5 Points)** Implement the Random Forest classifier using B = 1000 and $m=\\lceil \\sqrt{d} \\rceil$. You can use DecisionTreeClassifier from scikit-learn package together with the fit and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c425291",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "def train_and_predict_RF(B, m, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        B (int): number of trees\n",
    "        m (int): number of features to be randomly chosen at each split in the tree\n",
    "        X_train (Nxp matrix): training data\n",
    "        y_train (N vector): response values\n",
    "        X_test (N_test x p matrix): test data to be predicted\n",
    "    Outputs:\n",
    "        voting_test (N_test vector): 0-1 prediction for each individual in the test data\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "\n",
    "np.random.seed(1)\n",
    "voting_test = train_and_predict_RF(1000, int(np.round(np.sqrt(8))), X_train, y_train, X_test)\n",
    "acc_test = np.sum(y_test==voting_test)/len(y_test)\n",
    "print('The RF accuracy over the test sample is:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cabd99",
   "metadata": {},
   "source": [
    ">**Task 5: (5 Points)** Implement your functions from the labs to compute logistic regression and compare the test accuracy against Random Forest. Use gradient descent with 1000 iterations and a learning rate of $\\alpha=0.05$. Please do not make use of the scikit-learn implementation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here\n",
    "\n",
    "print('The LR accuracy over the test sample is:', 1 - np.mean(np.abs(predict(theta,X_test_LOG)-y_test_LOG)))\n",
    "print('The learnt coefficients are:', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2e528",
   "metadata": {},
   "source": [
    "In the following, we will study a variable importance measure designed for Random Forest: the Mean Decrease Accuracy (MDA), also known as the permutation importance measure. MDA is based on the following principle: if a variable is not influential in the model, rearranging the values it takes should not degrade prediction accuracy. If the predictor variable brings nothing but random noise, the prediction accuracy will likely not to be affected after the permutation. Every time an individual tree is grown over a bootstrap sample, accuracy in OOB observations is going to be computed. Also, accuracy in OOB observations after permuting the values of some predictor variable will be computed. The MDA of that predictor variable is obtained by averaging over all trees the differences of both accuracies, that is, accuracy in OOB observations before permuting minus accuracy in OOB observations after permuting.\n",
    "\n",
    ">**Task 6: (6 Points)** Complete the function below in order to compute the Mean Decrease Accuracy of each predictor variable at the same time of model fitting.  You can use DecisionTreeClassifier from scikit-learn package together with the fit and predict functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_importance_RF(B, m, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        B (int): number of trees\n",
    "        m (int): number of features to be randomly chosen at each split in the tree\n",
    "        X_train (Nxd matrix): training data\n",
    "        y_train (N vector): response values\n",
    "    Outputs:\n",
    "        variable_importance (d vector): MDA of each predictor variable\n",
    "    \"\"\"\n",
    "    #insert your code here\n",
    "\n",
    "np.random.seed(1)\n",
    "mda = variable_importance_RF(1000, int(np.round(np.sqrt(8))), X_train, y_train)\n",
    "plt.figure()\n",
    "plt.bar(np.arange(1,9), mda)\n",
    "plt.ylabel('Mean Decrease Accuracy')\n",
    "plt.xticks(np.arange(1,9), features, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cff93",
   "metadata": {},
   "source": [
    ">**Question 3: (3 Points)** Which is the most important predictor according to MDA? Is this information consistent with the fitted logistic regression parameters obtained from Task 5? In addition, discuss the sign of the parameter linked to this predictor variable in the logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453bce0",
   "metadata": {},
   "source": [
    "## C. Boosting Trees\n",
    "\n",
    "The motivation for boosting is to combine the outputs of many weak classifiers to produce a robust model. A weak classifier is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak classifiers. The predictions from all of them are then aggregated through a weighted majority vote to produce the final prediction. Contrary to bagging, boosting cannot be parallelized because each new version of the data that is dependent on the previous output. One of the most popoular boosting algorithm is AdaBoost, that we briefly summarize below.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "In AdaBoost, the data modifications at each boosting step consist of applying weights $w_1, \\ldots, w_N$ to each of the training observations. Initially all of the weights are set to $w_i = 1/N$, so that the first step simply trains the classifier on the data in the usual manner. For each successive iteration $b = 2, 3, \\ldots, B$ the observation weights are individually modified and the classification algorithm is reapplied to the weighted observations. At step $b$, those observations that were misclassified by the classifier induced at the previous step have their weights increased, whereas the weights are decreased for those that were classified correctly. Thus as iterations proceed, observations that are difficult to classify correctly receive ever-increasing influence. Each successive classifier is thereby forced to concentrate on those training observations that are missed by previous ones in the sequence.\n",
    "\n",
    "1. Initialize the observations weights $w_i = 1/N, i = 1,\\ldots,N$.\n",
    "2. For $b=1$ to $B$:\\\n",
    "a) Fit a weak classifier $C_b(x)$ to the training data using weights $w_i$.\\\n",
    "b) Compute the weighted error obtained with $C_b$, i.e., $e_b = \\frac{\\sum_{i=1}^N w_i I (y_i \\neq C_b(x_i))}{\\sum_{i=1}^N w_i}$.\\\n",
    "c) Compute $\\alpha_b = log((1-e_b)/e_b)$.\\\n",
    "d) Set $w_i$ to $w_i exp[\\alpha_b I (y_i \\neq C_b(x_i))], i = 1,\\ldots,N$.\n",
    "3. The final predictor function is $C(x) = \\frac{\\sum_{b=1}^B \\alpha_b C_b(x)}{\\sum_{b=1}^B \\alpha_b}$.\n",
    "\n",
    ">**Task 7: (8 Points)** Implement AdaBoost classifier using as weak classifier a DecisionTreeRegressor from scikit-learn package with one single split. You have to figure out how to pass the weight vector to the <code>fit</code> function. Compute at each boosting step the training and test accuracies so you can plot the learning curves as a function of the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "def train_and_predict_AdaBoost(B, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        B (int): number of boosting steps\n",
    "        X_train (Nxp matrix): training data\n",
    "        y_train (N vector): response training values\n",
    "        X_test (N_test x p matrix): test data to be predicted\n",
    "        y_test (N vector): response test values\n",
    "    Outputs:\n",
    "        acc_train (B vector): train accuracy per boosting step \n",
    "        acc_test (B vector): test accuracy per boosting step\n",
    "        w_all (NxB matrix): weights computed for all the samples per boosting step\n",
    "    \"\"\"\n",
    "    # insert your code here\n",
    "\n",
    "B = 1000\n",
    "acc_train, acc_test, w_all = train_and_predict_AdaBoost(B, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('The AdaBoost accuracy over the test sample is:', acc_test[B-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(acc_train, label=\"Training\")\n",
    "plt.plot(acc_test, label=\"Test\")\n",
    "plt.xlabel('Boosting steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd4e71",
   "metadata": {},
   "source": [
    ">**Task 8/Question 4: (5 Points)** Read in the simulated data set. It consists of two predictors and a binary response variable. Perform a train-test splitting and run your function train_and_predict_AdaBoost. Plot the average weights across all the boosting steps per each observation in the training sample by means of a barplot. What can you observe? Regarding to this observation, can you explore a potential problem of AdaBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = np.loadtxt(\"simulated.txt\", delimiter=',')\n",
    "\n",
    "n_test = 60\n",
    "X_sim_train = sim[:-n_test, 0:2]\n",
    "y_sim_train = sim[:-n_test, -1]\n",
    "X_sim_test = sim[-n_test:, 0:2]\n",
    "y_sim_test = sim[-n_test:, -1]\n",
    "\n",
    "#insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c788b8d",
   "metadata": {},
   "source": [
    ">**Question 5/Task 9: (5 Points)** Propose a modification of AdaBoost algorithm that aims to solve this problem, that is, propose a different probability distribution of the weights that you would use, and explain why this could work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b85e5",
   "metadata": {},
   "source": [
    "### XGBoost and LightGBM\n",
    "\n",
    "Gradient Boosting is a generalization of AdaBoost to a statistical framework that treats the training process as an additive model and allows arbitrary differentiable loss functions to be optimized using gradient descent.\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1603.02754.pdf\">Extreme Gradient Boosting</a> (XGBoost) and <a href=\"https://papers.nips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf\">Light Gradient Boosting Machines</a> (LightGBM) are two popular efficient implementations of Gradient Boosting. \n",
    "\n",
    ">**Question 6: (4 Points)** Describe the main characteristics of XGBoost and LightGBM in four sentences each. \n",
    "\n",
    ">**Task 10: (2 Points)** Implement both XGBoost and LightGBM with 1000 trees at maximum depth $1$, and compare their test accuracy over the Pima-indians-diabetes dataset to logistic regression, Random Forest and AdaBoost. You can use XGBClassifier from xgboost package, and LGBMClassifier from lightgbm package with a learning rate of 0'15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# insert your code here\n",
    "\n",
    "print('The XGBoost accuracy over the test sample is:', acc_test_xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# insert your code here\n",
    "\n",
    "print('The LightGBM accuracy over the test sample is:', acc_test_lgbmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be536b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
