{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 - Machine and Deep Learning\n",
    "## Lab 8 - Working with Text, Regularization and Recurrent Neural Networks\n",
    "\n",
    "In this lab we will be making some *sentiment analysis* with textual data from the IMDB dataset. In its simplest form, the *sentiment analysis* task consists in predicting the polarity of a text as positive or negative.\n",
    "\n",
    "First of all, let's take a look at the steps of text processing:\n",
    "\n",
    "1. Tokenization: break sentence into individual words\n",
    "    - Before: `\"PyTorch seems really easy to use!\"`\n",
    "    - After: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "2. Building vocabulary: build an index of words associated with unique numbers\n",
    "    - Before: `[\"PyTorch\", \"seems\", \"really\", \"easy\", \"to\", \"use\", \"!\"]`\n",
    "    - After: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "3. Convert to numerals: map words to unique numbers (indices)\n",
    "    - Before: `{\"Pytorch: 0, \"seems\": 1, \"really\": 2, ...}`\n",
    "    - After: `[0, 1, 2, ...]`\n",
    "4. Embedding look-up: map sentences (indices now) to fixed matrices\n",
    "    - ```[[0.1, 0.4, 0.3],\n",
    "       [0.8, 0.1, 0.5],\n",
    "       ...]```\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's download the IMDB dataset (~85MB) and create the train, test and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 25000 7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39913</th>\n",
       "      <td>Domestic Import was a great movie. I laughed t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13622</th>\n",
       "      <td>Midnight Madness is a movie that is unfortunat...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>Let me first off say that I am a believer of g...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38725</th>\n",
       "      <td>I think you would have to be from the USA to g...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38077</th>\n",
       "      <td>This is no doubt one of the worst movies I hav...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  label\n",
       "39913  Domestic Import was a great movie. I laughed t...  positive      1\n",
       "13622  Midnight Madness is a movie that is unfortunat...  positive      1\n",
       "875    Let me first off say that I am a believer of g...  negative      0\n",
       "38725  I think you would have to be from the USA to g...  positive      1\n",
       "38077  This is no doubt one of the worst movies I hav...  negative      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('./data/IMDB Dataset.csv')\n",
    "data['label']=data['sentiment'].replace(['positive', 'negative'], [1,0])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.5, random_state=1337)\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.3, random_state=1337)\n",
    "\n",
    "print(len(train), len(test), len(valid))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the *vocabulary* (step 2 above), mapping every word into a numerical id. We consider only the text in the training set for this step. We limit the vocabulary to the 1000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic Import was a great movie. I laughed the whole time. It was funny on so many levels from the crazy outfits to the hilarious situations. The acting was great. Alla Korot, Larry Dorf, Howard Hesseman, and all the others did an awesome job. Because it is an independent film written by a first-time writer, it doesn't have the clich√©s that are expected of other comedies, which was such a relief. It was a unique and interesting and you fall in love with the characters and the heart-warming story. I heard it was based on a true story? If so, then that is hilarious (and amazing!). I highly recommend this movie.\n",
      "[0, 0, 17, 6, 83, 21, 3, 13, 0, 2, 223, 68, 3, 11, 17, 164, 27, 43, 115, 0, 44, 2, 905, 0, 8, 2, 572, 0, 3, 2, 128, 17, 83, 3, 0, 0, 4, 0, 0, 4, 0, 0, 4, 5, 38, 2, 402, 121, 41, 0, 296, 3, 95, 11, 10, 41, 0, 23, 444, 40, 6, 0, 746, 4, 11, 159, 9, 28, 33, 2, 0, 15, 31, 844, 7, 84, 0, 4, 69, 17, 150, 6, 0, 3, 11, 17, 6, 983, 5, 229, 5, 26, 798, 12, 118, 19, 2, 111, 5, 2, 0, 73, 3, 13, 554, 11, 17, 461, 27, 6, 293, 73, 54, 50, 43, 4, 100, 15, 10, 572, 25, 5, 481, 36, 24, 3, 13, 534, 378, 14, 21, 3]\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "words=[]\n",
    "num_words = 1000\n",
    "\n",
    "for text in train['review']:\n",
    "    tokens=tokenizer(text)\n",
    "    words.extend(tokens)\n",
    "\n",
    "top_1k = dict(Counter(words).most_common(1000))\n",
    "vocab = torchtext.vocab.vocab(top_1k, specials = ['<unk>', '<pad>'])\n",
    "#uncomment the following commented lines if you're using torchtext version 0.11\n",
    "#older versions do not support this way to build a vocabulary\n",
    "#vocab = torchtext.vocab.vocab(top_1k)\n",
    "#vocab.append_token('<unk>')\n",
    "#vocab.append_token('<pad>')\n",
    "vocab.set_default_index(vocab['<unk>']) #default index used when an unknown words is found\n",
    "\n",
    "print(train['review'].iloc[0])\n",
    "print(vocab.forward(tokenizer(train['review'].iloc[0]))) #example of how a sentence is transformed into a sequence of numerical IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform all reviews to vectors of word IDs. We also need to make all reviews of the same length. We set max_length to 80, therefore we'll cut reviews that exceed 80 words and pad those that are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=80\n",
    "\n",
    "def vectorize_sentences(reviews, max_len):\n",
    "    vectors=[]\n",
    "    for text in reviews:\n",
    "        tokens=tokenizer(text)\n",
    "        v=vocab.forward(tokens)\n",
    "        if len(v) > max_len : v = v[:max_len]\n",
    "        if len(v) < max_len : #padding\n",
    "            tmp = np.full(max_len, vocab['<pad>'])\n",
    "            tmp[0:len(v)]=v \n",
    "            v = tmp\n",
    "        vectors.append(np.array(v))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "train_X = vectorize_sentences(train['review'], max_len)\n",
    "test_X = vectorize_sentences(test['review'], max_len)\n",
    "val_X = vectorize_sentences(valid['review'], max_len)\n",
    "\n",
    "train_y = np.array(train['label']).reshape(-1,1)\n",
    "test_y = np.array(test['label']).reshape(-1,1)\n",
    "val_y = np.array(valid['label']).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and load the batches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# create tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).float().to(device))\n",
    "validset = TensorDataset(torch.from_numpy(val_X).to(device), torch.from_numpy(val_y).float().to(device))\n",
    "testset = TensorDataset(torch.from_numpy(test_X).to(device), torch.from_numpy(test_y).float().to(device))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce now a fully connected NN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_len):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(max_len*embedding_dim, hidden_dim) \n",
    "        \n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.view(-1, max_len*embedding_dim)\n",
    "        # Linear function\n",
    "        out = self.fc1(embedded)\n",
    "\n",
    "        # Non-linear activation function\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # Here we use a final sigmoid function as we are using BCE loss which does not implement it\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = num_words + 2 #add 2 for <unk> and <pad> symbols\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, embedding_dim, hidden_dim, output_dim, max_len)\n",
    "\n",
    "# Push model to CUDA device if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing problems are affected by the problem of features that occur infrequently. E.g., it is a lot less likely that we will see the word *phantasmagorical* than the word *reading*. Parameters associated with infrequent features only receive meaningful updates whenever these features occur. Given a decreasing learning rate we might end up in a situation where the parameters for common features converge rather quickly to their optimal values, whereas for infrequent features we are still short of observing them sufficiently frequently before their optimal values can be determined. In other words, the learning rate either decreases too slowly for frequent features or too quickly for infrequent ones.\n",
    "\n",
    "A solution to this problem are *adaptive learning rates*. For instance, **Adagrad** is an optimization technique based on the accumulation of squared gradients.\n",
    "\n",
    "Here we use the notation $\\mathbf{g}_t = \\nabla_{\\mathbf{w}} l(y_t, f(\\mathbf{x}_t, \\mathbf{w}))$ for the gradient of the loss function at time $t$. We use the variable $\\mathbf{s}_t$ to accumulate past gradient variance.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & = \\mathbf{s}_{t-1} + \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{w}_t & = \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n",
    "\\end{aligned}$$\n",
    "\n",
    "However Adagrad has some issues and for our case we are going to use an improved version called <a href=\"https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\">**RMSprop**</a>. It uses a moving average of squared gradients to normalize the gradient. This normalization balances the step size (momentum), decreasing the step for large gradients to avoid exploding and increasing the step for small gradients to avoid vanishing:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & \\leftarrow \\gamma \\mathbf{s}_{t-1} + (1 - \\gamma) \\mathbf{g}_t^2, \\\\\n",
    "    \\mathbf{w}_t & \\leftarrow \\mathbf{w}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\mathbf{g}_t.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The constant $\\epsilon > 0$ is typically set to $10^{-6}$ to ensure that we do not suffer from division by zero or overly large step sizes. $\\gamma$ is usually set to 0.9.\n",
    "\n",
    "The following block sets up the network with <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\">Binary Cross Entropy</a> loss and RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups of parameters 5\n",
      "--------------------------------------------------\n",
      "torch.Size([1002, 100])\n",
      "torch.Size([32, 8000])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Number of groups of parameters\n",
    "print('Number of groups of parameters {}'.format(len(list(model.parameters()))))\n",
    "print('-'*50)\n",
    "# Print parameters\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function carries out the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optizmizer, loss_criterion):\n",
    "    iter = 0\n",
    "    num_epochs = 10\n",
    "    history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (samples, labels) in enumerate(train_loader):\n",
    "            # Training mode\n",
    "            model.train()\n",
    "\n",
    "            # Load samples\n",
    "            samples = samples.view(-1, max_len).to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            outputs = model(samples)\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                # Get training statistics\n",
    "                train_loss = loss.data.item()\n",
    "\n",
    "                # Testing mode\n",
    "                model.eval()\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for samples, labels in valid_loader:\n",
    "                    # Load samples\n",
    "                    samples = samples.view(-1, max_len).to(device)\n",
    "                    labels = labels.view(-1).to(device)\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(samples)\n",
    "\n",
    "                    # Val loss\n",
    "                    val_loss = criterion(outputs.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "                    predicted = outputs.ge(0.5).view(-1)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum().item()\n",
    "                    # correct = (predicted == labels.byte()).int().sum().item()\n",
    "\n",
    "                accuracy = 100. * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format(iter, train_loss, val_loss.item(), round(accuracy, 2)))\n",
    "\n",
    "                # Append to history\n",
    "                history_val_loss.append(val_loss.data.item())\n",
    "                history_val_acc.append(round(accuracy, 2))\n",
    "                history_train_loss.append(train_loss)\n",
    "\n",
    "                # Save model when accuracy beats best accuracy\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    # We can load this best model on the validation set later\n",
    "                    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    return (history_train_acc, history_val_acc, history_train_loss, history_val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the losses for the testing and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history_train_loss, history_val_loss):\n",
    "    # Set plotting style\n",
    "    #plt.style.use(('dark_background', 'bmh'))\n",
    "    plt.style.use('bmh')\n",
    "    plt.rc('axes', facecolor='none')\n",
    "    plt.rc('figure', figsize=(16, 4))\n",
    "\n",
    "    # Plotting loss graph\n",
    "    plt.plot(history_train_loss, label='Train')\n",
    "    plt.plot(history_val_loss, label='Validation')\n",
    "    plt.title('Loss Graph')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the behaviour of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 | Train Loss: 0.6810848712921143 | Val Loss: 0.6830800771713257 | Val Accuracy: 51.77\n",
      "Iter: 200 | Train Loss: 0.7062846422195435 | Val Loss: 0.7088586688041687 | Val Accuracy: 55.13\n",
      "Iter: 300 | Train Loss: 0.5619038343429565 | Val Loss: 0.5862455368041992 | Val Accuracy: 56.16\n",
      "Iter: 400 | Train Loss: 0.6861127018928528 | Val Loss: 0.8024255633354187 | Val Accuracy: 58.43\n",
      "Iter: 500 | Train Loss: 0.5844285488128662 | Val Loss: 0.5503729581832886 | Val Accuracy: 59.95\n",
      "Iter: 600 | Train Loss: 0.42505693435668945 | Val Loss: 0.506284236907959 | Val Accuracy: 60.69\n",
      "Iter: 700 | Train Loss: 0.5001639127731323 | Val Loss: 1.0521844625473022 | Val Accuracy: 60.71\n",
      "Iter: 800 | Train Loss: 0.3493354320526123 | Val Loss: 0.8583499789237976 | Val Accuracy: 61.29\n",
      "Iter: 900 | Train Loss: 0.2788204848766327 | Val Loss: 0.5286469459533691 | Val Accuracy: 61.79\n",
      "Iter: 1000 | Train Loss: 0.22440889477729797 | Val Loss: 0.4389880299568176 | Val Accuracy: 62.29\n",
      "Iter: 1100 | Train Loss: 0.139530748128891 | Val Loss: 0.8306031227111816 | Val Accuracy: 62.12\n",
      "Iter: 1200 | Train Loss: 0.09153257310390472 | Val Loss: 1.8574284315109253 | Val Accuracy: 62.09\n",
      "Iter: 1300 | Train Loss: 0.160803884267807 | Val Loss: 1.6241897344589233 | Val Accuracy: 62.05\n",
      "Iter: 1400 | Train Loss: 0.07898668199777603 | Val Loss: 1.3064755201339722 | Val Accuracy: 62.52\n",
      "Iter: 1500 | Train Loss: 0.08766379952430725 | Val Loss: 0.8236667513847351 | Val Accuracy: 62.11\n",
      "Iter: 1600 | Train Loss: 0.04784732684493065 | Val Loss: 3.0247275829315186 | Val Accuracy: 62.67\n",
      "Iter: 1700 | Train Loss: 0.07148405909538269 | Val Loss: 1.9309990406036377 | Val Accuracy: 62.19\n",
      "Iter: 1800 | Train Loss: 0.03531758487224579 | Val Loss: 1.5697822570800781 | Val Accuracy: 62.64\n",
      "Iter: 1900 | Train Loss: 0.0175765473395586 | Val Loss: 4.063948631286621 | Val Accuracy: 62.51\n",
      "Iter: 2000 | Train Loss: 0.010891140438616276 | Val Loss: 1.2392268180847168 | Val Accuracy: 62.24\n",
      "Iter: 2100 | Train Loss: 0.009803703054785728 | Val Loss: 2.2477359771728516 | Val Accuracy: 62.95\n",
      "Iter: 2200 | Train Loss: 0.02460525743663311 | Val Loss: 2.094733715057373 | Val Accuracy: 62.4\n",
      "Iter: 2300 | Train Loss: 0.018146289512515068 | Val Loss: 1.3136844635009766 | Val Accuracy: 62.25\n",
      "Iter: 2400 | Train Loss: 0.010328478179872036 | Val Loss: 1.0212571620941162 | Val Accuracy: 62.68\n",
      "Iter: 2500 | Train Loss: 0.015589412301778793 | Val Loss: 2.509908676147461 | Val Accuracy: 62.83\n",
      "Iter: 2600 | Train Loss: 0.0027822498232126236 | Val Loss: 0.616747260093689 | Val Accuracy: 62.69\n",
      "Iter: 2700 | Train Loss: 0.009017586708068848 | Val Loss: 2.2104854583740234 | Val Accuracy: 62.43\n"
     ]
    }
   ],
   "source": [
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mplot_losses\u001b[1;34m(history_train_loss, history_val_loss)\u001b[0m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss Graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m---> 13\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\IPython\\core\\formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\IPython\\core\\formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[0;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[0;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m   2340\u001b[0m     )\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[1;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[0;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axes\\_base.py:3028\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3025\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m spine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspines\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   3026\u001b[0m         artists\u001b[38;5;241m.\u001b[39mremove(spine)\n\u001b[1;32m-> 3028\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_title_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxison:\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axis_map\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axes\\_base.py:2972\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2970\u001b[0m top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(top, bb\u001b[38;5;241m.\u001b[39mymax)\n\u001b[0;32m   2971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[1;32m-> 2972\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myaxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update offsetText\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_text():\n\u001b[0;32m   2974\u001b[0m         bb \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39moffsetText\u001b[38;5;241m.\u001b[39mget_tightbbox(renderer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axis.py:1337\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[1;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m   1335\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[1;32m-> 1337\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axis.py:2614\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2612\u001b[0m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m \u001b[38;5;66;03m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[1;32m-> 2614\u001b[0m bboxes, bboxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2615\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mget_position()\n\u001b[0;32m   2616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axis.py:2149\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2147\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2148\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[1;32m-> 2149\u001b[0m tlb, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2150\u001b[0m bboxes\u001b[38;5;241m.\u001b[39mextend(tlb)\n\u001b[0;32m   2151\u001b[0m bboxes2\u001b[38;5;241m.\u001b[39mextend(tlb2)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axis.py:1316\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[1;34m(self, ticks, renderer)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[0;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\axis.py:1316\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[0;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\text.py:959\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[1;34m(self, renderer, dpi)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m--> 959\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[0;32m    961\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\text.py:378\u001b[0m, in \u001b[0;36mText._get_layout\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    375\u001b[0m ys \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Full vertical extent of font, including ascenders and descenders:\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m _, lp_h, lp_d \u001b[38;5;241m=\u001b[39m \u001b[43m_get_text_metrics_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTeX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_usetex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m min_dy \u001b[38;5;241m=\u001b[39m (lp_h \u001b[38;5;241m-\u001b[39m lp_d) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linespacing\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\text.py:97\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache\u001b[1;34m(renderer, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_text_metrics_with_cache_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\text.py:105\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache_impl\u001b[1;34m(renderer_ref, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_metrics_with_cache_impl\u001b[39m(\n\u001b[0;32m    103\u001b[0m         renderer_ref, text, fontprop, ismath, dpi):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# dpi is unused, but participates in cache invalidation (via the renderer).\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrenderer_ref\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_width_height_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\backends\\backend_agg.py:233\u001b[0m, in \u001b[0;36mRendererAgg.get_text_width_height_descent\u001b[1;34m(self, s, prop, ismath)\u001b[0m\n\u001b[0;32m    229\u001b[0m     ox, oy, width, height, descent, font_image \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmathtext_parser\u001b[38;5;241m.\u001b[39mparse(s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpi, prop)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m width, height, descent\n\u001b[1;32m--> 233\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_font\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m font\u001b[38;5;241m.\u001b[39mset_text(s, \u001b[38;5;241m0.0\u001b[39m, flags\u001b[38;5;241m=\u001b[39mget_hinting_flag())\n\u001b[0;32m    235\u001b[0m w, h \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_width_height()  \u001b[38;5;66;03m# width and height of unrotated string\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\backends\\backend_agg.py:267\u001b[0m, in \u001b[0;36mRendererAgg._prepare_font\u001b[1;34m(self, font_prop)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_font\u001b[39m(\u001b[38;5;28mself\u001b[39m, font_prop):\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    Get the `.FT2Font` for *font_prop*, clear its buffer, and set its size.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[43mget_font\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fontManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_fonts_by_props\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont_prop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     font\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    269\u001b[0m     size \u001b[38;5;241m=\u001b[39m font_prop\u001b[38;5;241m.\u001b[39mget_size_in_points()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\_api\\deprecation.py:297\u001b[0m, in \u001b[0;36mrename_parameter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     warn_deprecated(\n\u001b[0;32m    293\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas been renamed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m since Matplotlib \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the old name will be dropped %(removal)s.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m     kwargs[new] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(old)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\font_manager.py:1523\u001b[0m, in \u001b[0;36mget_font\u001b[1;34m(font_filepaths, hinting_factor)\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hinting_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1521\u001b[0m     hinting_factor \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext.hinting_factor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 1523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_font\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# must be a tuple to be cached\u001b[39;49;00m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhinting_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_kerning_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext.kerning_factor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# also key on the thread ID to prevent segfaults with multi-threading\u001b[39;49;00m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ident\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\matplotlib\\font_manager.py:1464\u001b[0m, in \u001b[0;36m_get_font\u001b[1;34m(font_filepaths, hinting_factor, _kerning_factor, thread_id)\u001b[0m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_font\u001b[39m(font_filepaths, hinting_factor, \u001b[38;5;241m*\u001b[39m, _kerning_factor, thread_id):\n\u001b[0;32m   1463\u001b[0m     first_fontpath, \u001b[38;5;241m*\u001b[39mrest \u001b[38;5;241m=\u001b[39m font_filepaths\n\u001b[1;32m-> 1464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mft2font\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFT2Font\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_fontpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhinting_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fallback_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[43mft2font\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFT2Font\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhinting_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_kerning_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_kerning_factor\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrest\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_kerning_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_kerning_factor\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ques 1**: What can you deduce from this graph? Is the network working as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization techniques are required to reduce overfitting (i.e. the predictor fits too closely to the training data and does not generalize well to new data). Overfitting tends to occur when we have little data and a complex hypothesis class.\n",
    "\n",
    "A first idea for regularization is to introduce a *penalty term*, which makes it harder for the optimizer to return an overly flexible predictor.\n",
    "\n",
    "We will start by adding a regularization term to our (point-wise) loss:\n",
    "\n",
    "$$ loss_R = loss + \\frac{\\lambda}{2}\\left\\Vert W\\right\\Vert _{2}^{2}$$\n",
    "\n",
    "The gradient for the regularized loss is therefore:\n",
    "\n",
    "$$\\nabla_W loss_R = \\nabla_W loss + \\lambda W$$\n",
    "\n",
    "With pytorch, adding a regularization term is done on the optimization function by adding the parameter *weight_decay* (corresponding to the above $\\lambda$) to the call to the optimizer.\n",
    "\n",
    "> **Task 1**: set the optimizer *weight_decay* to 0.005 and re-train the model, studying its behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to take a look at the parameters of the model before and after regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_parameters(mdl):\n",
    "    weights = torch.Tensor().to(device)\n",
    "    for param_group in list(mdl.parameters()):\n",
    "        weights = torch.cat((param_group.view(-1), weights))\n",
    "    ws = weights.detach().cpu().numpy()\n",
    "    plt.hist(ws.reshape(-1), range=(-.5, .5), bins=501)\n",
    "\n",
    "look_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model class and assign to object\n",
    "model = FeedforwardNeuralNetModel(input_dim, embedding_dim, hidden_dim, output_dim, max_len)\n",
    "model.to(device)\n",
    "\n",
    "#enter here the code for Task 1\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, val_loss)\n",
    "look_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Another solution for regularization in neural network consists in applying **Dropout**: at each step of training, a new subnetwork is selected. As a result an adaptation appears in the final network only if it exists in a sufficient part of the training data.\n",
    "\n",
    "<img src=\"figures/dropout.png\" alt=\"dropout\" width=\"400\"/><font size=\"1\">Image from (Srivastava et al. ,2014) Dropout: A Simple Way to Prevent Neural Networks from\n",
    "Overfitting, JMLR</font>\n",
    "\n",
    "We need to introduce a probability $p$ of deactivating a neuron in the hidden layers. So, if we have $n$ neurons in layer $l$, we expect to have $n * (1-p)$ active neurons.\n",
    "\n",
    "Therefore the output of a generic layer becomes:\n",
    "\n",
    "$$ z^l = \\alpha(W_l^\\top \\delta z^{l-1} + b_l) $$\n",
    "\n",
    "with $\\delta \\sim Bernoulli(p)$ and $\\alpha(x)$ an activation function.\n",
    "\n",
    "> **Task 2** : add <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\">dropout</a> to the neural network above, between the two fully connected layers, with p=0.8; re-train the new network and observe the result (note: use the optimizer without weight_decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModelWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, max_len):\n",
    "        super(FeedforwardNeuralNetModelWithDropout, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(max_len*embedding_dim, hidden_dim) \n",
    "        \n",
    "        #insert here your code\n",
    "        \n",
    "        \n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        #print(\"embedding for word \", vocab.lookup_token(83))\n",
    "        #print(self.embedding(torch.Tensor([83]).int()))\n",
    "        \n",
    "        embedded = embedded.view(-1, max_len*embedding_dim)\n",
    "        # Linear function\n",
    "        out = self.fc1(embedded)\n",
    "\n",
    "        # Non-linearity\n",
    "        out = torch.relu(out)\n",
    "        \n",
    "        #insert here your code\n",
    "        \n",
    "        \n",
    "        # Take note here use a final sigmoid function so your loss should not go through sigmoid again as we are using BCE loss.\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "    \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here your code\n",
    "# Instantiate model class and assign to object\n",
    "model_dropout = \n",
    "\n",
    "model_dropout.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model_dropout, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train your model here\n",
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM to classify textual data\n",
    "\n",
    "A fully connected neural network is not an ideal way of processing textual data. Recurrent Neural Networks are more useful for this task since they are more adequate to work on sequential data such as text.\n",
    "\n",
    "A particular type of RNN is the Long Short-Term Memory, or <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">LSTM</a>.\n",
    "\n",
    "> **Task 3** : complete the following code to implement a single layer, unidirectional LSTM with a fully connected layer taking the final hidden states of the LSTM as input. Note that you have to set the parameter *batch_first*=True for the LSTM layer as our dataset comes in this format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        \"\"\"\n",
    "        vocab_size: (int) size of the vocabulary - required by embeddings\n",
    "        embed_dim: (int) size of embeddings\n",
    "        hidden_dim: (int) number of hidden units\n",
    "        num_class: (int) number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        #enter here your code\n",
    "        \n",
    "       \n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        #ENTER HERE YOUR CODE\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 4**: run the LSTM model on the IMDB dataset and verify whether it is overfitting or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your code here\n",
    "\n",
    "lstm = \n",
    "lstm.to(device)\n",
    "\n",
    "optimizer = \n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(lstm, optimizer, criterion)\n",
    "\n",
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 5**: implement a dropout layer between the LSTM and the fully connected layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at Word Embeddings\n",
    "\n",
    "The word embeddings calculated during training can be extracted from the models and they can be used to calculate a semantic similarity between words.\n",
    "\n",
    "> **Task 6** : include the following code into the LSTM implementation\n",
    "\n",
    "```\n",
    "def get_embedding_for(self, w):\n",
    "    idx = vocab.lookup_indices([w])\n",
    "    return self.embedding(torch.Tensor(idx).int())\n",
    "````\n",
    "> and test the following block:\n",
    "\n",
    "you should see that words that are intuitively semantically closer have higher similarity scores (according to cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1=(lstm.get_embedding_for(\"great\").detach().numpy()).reshape(1,-1)\n",
    "v2=(lstm.get_embedding_for(\"bad\").detach().numpy()).reshape(1,-1)\n",
    "v3=(lstm.get_embedding_for(\"awful\").detach().numpy()).reshape(1,-1)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = dot(v1, v2.T)/(norm(v1)*norm(v2))\n",
    "\n",
    "print(\"cosine sim between great and bad\", cos_sim)\n",
    "\n",
    "cos_sim = dot(v2, v3.T)/(norm(v2)*norm(v3))\n",
    "\n",
    "print(\"cosine sim between bad and awful\", cos_sim)\n",
    "\n",
    "cos_sim = dot(v1, v3.T)/(norm(v1)*norm(v3))\n",
    "\n",
    "print(\"cosine sim between great and awful\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
